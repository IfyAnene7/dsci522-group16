{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Exploratory Data Analysis of the Coimbra Breast Cancer data Data Set._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used in this project consists of anthropometric data and parameters gathered in a standard blood analysis. This dataset was created by Miguel Patrício, José Pereira, Joana Crisóstomo, Paulo Matafome, Raquel Seiça, Francisco Caramelo, all from the Faculty of Medicine of the University of Coimbra and also Manuel Gomes from the University Hospital Centre of Coimbra (Patrício et al., 2018). The dataset was sourced from the UCI Machine Learning Repository (Dua and Graff 2017) and it can be found [here](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Coimbra), particularly [this file](https://archive.ics.uci.edu/ml/machine-learning-databases/00451/dataR2.csv). Each row in this dataset represents a set of observations of individual patients and each column represents a variable. In this dataset, there are 116 observations and 9 features which are all numerical. There are zero observations with missing values for each class in the dataset. The target column is a binary dependent variable, which indicates the presence (Classification = 2) or absence (Classification = 1) of breast cancer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis checklist:\n",
    "\n",
    "- Formulate the question\n",
    "- Read in the data\n",
    "- Check the packaging\n",
    "- Look at the top and the bottom of your data\n",
    "- Make a plot\n",
    "- Follow up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulate the Question: \n",
    "\n",
    "Given the clinical and anthropometric data available, predict if a patient have breast cancer or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Classifiers \n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# train test split and cross validation\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "\n",
    "# Feature and model selection metrics\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    plot_confusion_matrix,\n",
    ")\n",
    "\n",
    "# Preprocessing and pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline, make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import (\n",
    "    FunctionTransformer,\n",
    "    Normalizer,\n",
    "    StandardScaler,\n",
    "    normalize,\n",
    "    scale,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data and Check the packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_df = pd.read_csv(\"../data/raw/dataR2.csv\")\n",
    "\n",
    "bc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Workflow to which we should adhere.\n",
    "\n",
    "To avoid breaking the golden rule and hence optaining an optimistic estimate of our model's performance when computing scores (which is bad), we have decided to split our dataset before performing an exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(bc_df, test_size = 0.2, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe(include = \"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train_df.drop(columns = [\"Classification\"]).select_dtypes(include = np.number)\n",
    "\n",
    "for feat in features:\n",
    "    eda_fig = plt.gcf()\n",
    "    ax = train_df.groupby(\"Classification\")[feat].plot.hist(bins = 20, alpha = 0.4, legend = True)\n",
    "    plt.xlabel(feat)\n",
    "    plt.title(\"Histogram of \" + feat)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow up:\n",
    "\n",
    "Looking at the graphs developed above, there seems to be some interesting features (such as Glucose, Insulin, HOMA, and Resistin) which can be used to predict the presence or absence of breast cancer. Therefore, we plan on exploring classification evaluation metrics, developing a baseline model, exploring more complicated models, choosing a model based on our evaluation metrics, and performing hyperparameter optimization of the model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train_df.drop(columns=[\"Classification\"]), train_df[\"Classification\"]\n",
    "\n",
    "X_test, y_test = test_df.drop(columns=[\"Classification\"]), test_df[\"Classification\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "numeric_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop(columns = [\"Classification\"])\n",
    "y_train = train_df[\"Classification\"]\n",
    "\n",
    "\n",
    "X_test = test_df.drop(columns = [\"Classification\"])\n",
    "y_test = test_df[\"Classification\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Given that we only have numerical features and provided that there are no missing values, therefore, we decided to create a machine learning pipeline that only scales the numerical features, so that we can get appropriate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since there is no missing values we do not need impute the data\n",
    "numeric_transformer = make_pipeline(StandardScaler())   \n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (numeric_transformer, numeric_features)\n",
    ")\n",
    "\n",
    "preprocessor.fit(X_train);             # Calling fit to examine all the transformers\n",
    "preprocessor.named_transformers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a baseline model and Multiple models\n",
    "\n",
    "We decided to fit a baseline model to give us an idea of what a minimum score would look like. Furthermore, we attempted to fit different classifiers to observe how they perform on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"DummyClassifier\": DummyClassifier(strategy=\"most_frequent\"),\n",
    "    \"Decision tree\": DecisionTreeClassifier(),\n",
    "    \"kNN\": KNeighborsClassifier(),\n",
    "    \"RBF SVM\": SVC(),\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "results = {}\n",
    "\n",
    "scoring = ['recall', 'accuracy', 'precision', 'f1']\n",
    "\n",
    "for name, classifier in classifiers.items():\n",
    "    pipe_classifier = make_pipeline(preprocessor, classifier)\n",
    "    scores = cross_validate(pipe_classifier, X_train, y_train, return_train_score = True, scoring = scoring)\n",
    "    results = {name: pd.DataFrame(scores).mean().tolist()}\n",
    "    results_dict.update(results)\n",
    "    \n",
    "pd.DataFrame(results_dict, index = scores.keys()).round(4).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Results\n",
    "\n",
    "The problem statement attempts to predict the presence or absence of breast cancer given the clinical and anthropometric data available. Based on this, we are trying to reduce the amount of false negatives that our model predicts (i.e we care more about accurately predicting the presence of breast cancer), therefore an important evaluation metric is the recall score. Analysing the table above, we can see that the dummy classifier model has the lowest recall scores (0) therefore we can completely ignore this model. Looking at the scores of the decision tree classifier, it has a high train scores for recall, accuracy and f1, however, it has much lower validation (\"test\") scores, this is indicative of overfitting on the training data, hence, we can eliminate this classifier. The Random Forest classifier can also be discarded using an analysis equivalent to the one used in discarding the Decision Tree classifier (i.e really high train scores, much lower validation (\"test\") scores).\n",
    "\n",
    "Furthermore, the kNearest neighbours classifier, Support Vector Machine (SVM) using the Radial Basis Function (RBF) kernel and the logistic regression classifier have similar train and validation scores, however, it is observed that the fundamental tradeoff (i.e the difference between the train and validation errors as train score increases) is minimal when considering the Logistic regression classifier. The logistic regression classifier also has the highest validation f1 (\"test_f1\") score. This is an extremely promising classifier! Moreover, further down the line we might attempt to optimise the hyperparameters of the classifier chosen thus, the f1 score is also important because it gives us a score to use when optimizing the hyperparameters and it also combines both the recall and precision scores.\n",
    "\n",
    "Finally, predicated on the aforementioned analysis, the logistic regression is the optimum model chosen to solve this question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _References_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patrício, M., Pereira, J., Crisóstomo, J., Matafome, P., Gomes, M., Seiça, R. and Caramelo, F., 2018. Using Resistin, glucose, age and BMI to predict the presence of breast cancer. BMC Cancer, 18(1). https://doi.org/10.1186/s12885-017-3877-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dua, Dheeru, and Casey Graff. 2017. “UCI Machine Learning Repository.” University of California, Irvine, School of Information; Computer Sciences. http://archive.ics.uci.edu/ml."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:571]",
   "language": "python",
   "name": "conda-env-571-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
